# Vietnamese-English Translator

Complete Neural Machine Translation system using Transformer architecture.

## ğŸ“ Project Structure

```
ViEn_Translator/
â”‚
â”œâ”€â”€ models_best/             # MODEL ARCHITECTURE ONLY
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ config.py            # TransformerConfig
â”‚   â”œâ”€â”€ transformer.py       # BestTransformer (main model)
â”‚   â”œâ”€â”€ encoder.py           # Pre-LN encoder
â”‚   â”œâ”€â”€ decoder.py           # Pre-LN decoder
â”‚   â”œâ”€â”€ attention.py         # Multi-head attention
â”‚   â”œâ”€â”€ feed_forward.py      # Feed-forward network
â”‚   â”œâ”€â”€ embeddings.py        # Embedding layers
â”‚   â”œâ”€â”€ positional_encoding.py  # Position encoding
â”‚   â”œâ”€â”€ layer_norm.py        # LayerNorm & RMSNorm
â”‚   â”œâ”€â”€ beam_search.py       # Beam search decoder
â”‚   â””â”€â”€ label_smoothing.py   # Label smoothing loss
â”‚
â”œâ”€â”€ trainer/                 # TRAINING & INFERENCE
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train.py             # Training script (viâ†’en)
â”‚   â”œâ”€â”€ inference.py         # Translation inference
â”‚   â””â”€â”€ evaluate.py          # BLEU evaluation
â”‚
â”œâ”€â”€ utils/                   # DATA PROCESSING
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ data_processing.py   # DataProcessor, Dataset, collate_fn
â”‚
â”œâ”€â”€ SentencePiece-from-scratch/  #  TOKENIZER
â”‚   â”œâ”€â”€ tokenizer_models/
â”‚   â”‚   â”œâ”€â”€ vocabulary.txt   # 32k vocab
â”‚   â”‚   â””â”€â”€ metadata.txt
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ data/                     # ğŸ“Š DATASETS
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ train_tokenized.pkl
â”‚       â”œâ”€â”€ validation_tokenized.pkl
â”‚       â””â”€â”€ test_tokenized.pkl
â”‚
â”œâ”€â”€ checkpoints/              # ğŸ’¾ SAVED MODELS
â”‚   â”œâ”€â”€ best_model_vi2en/
â”‚   â””â”€â”€ best_model_bidirectional/
â”‚
â”œâ”€â”€ config.py                 # Global config
â””â”€â”€ README.md                 # This file
```

## âœ¨ Features

### Model Architecture

- **Pre-Layer Normalization** - More stable training
- **Weight Tying** - Decoder embedding = output projection
- **Label Smoothing** - Better generalization (0.1)
- **Beam Search** - High-quality inference with length penalty
- **Multi-Query Attention** - Faster inference (optional)
- **Gradient Clipping** - Prevent gradient explosion

### Training Features

- **Warmup LR Scheduler** - Linear warmup + inverse sqrt decay
- **Mixed Precision Training** - Faster with modern GPUs
- **Checkpoint Management** - Auto-save best model
- **Training Curves** - Automatic plotting
- **Resume Training** - Load from checkpoint

### Data Processing

- **SentencePiece Tokenizer** - 32,000 BPE tokens
- **Cached Tokenization** - Fast data loading (.pkl files)
- **Proper Masking** - Padding mask + causal mask
- **Bidirectional Support** - Train single model for both directions

## ğŸš€ Quick Start

### 0. Data preprocessing

```bash
python utils/data_processing.py
```

### 1. Train Vietnamese â†’ English

```bash
python trainer/train.py
```

**Configuration:**

- Model: Base (512d, 6 layers, 65M params)
- Batch size: From `config.Config.BATCH_SIZE`
- Max length: From `config.Config.MAX_LEN`
- Device: Auto-detect CUDA/CPU
- Saves to: `checkpoints/best_model_vi2en/`

### 2. Inference (Translation)

#### a) Translate a file (batch, recommended)

```bash
python translate_test_file.py
```

This script translates `data/processed/test.en` to `data/processed/test_predict.vi` using the trained model. Output is also saved as a JSONL file for evaluation.

#### b) Fast inference from pre-tokenized data

```bash
python translate_from_tokenized.py --input data/processed/test_tokenized.pkl --output data/processed/test_predict.vi
```

See script for more options (batch size, device, etc). This is fastest for large test sets.

#### c) Translate a single sentence (interactive)

```python
from trainer.inference import Translator
translator = Translator(
    checkpoint_path='checkpoints/best_model_vi2en/best_model.pt',
    tokenizer_dir='SentencePiece-from-scratch/tokenizer_models',
    device='cuda'  # or 'cpu'
)
print(translator.translate("Xin chÃ o tháº¿ giá»›i"))
```

### 3. Evaluate BLEU Score

#### a) Evaluate from JSONL output (recommended)

```bash
python evaluate_jsonl.py
```

This compares the predicted translations in `data/processed/ep13_test_predict.jsonl` with the reference `data/processed/test.vi` and prints BLEU-1 to BLEU-4 scores. Results are saved as a JSON file.

#### b) Custom evaluation

You can modify `evaluate_jsonl.py` to point to your own prediction/reference files.

## ğŸ› ï¸ Workflow Guide

### 1. Prepare Data

- **Train tokenizer:**

  ```bash
  cd SentencePiece-from-scratch
  python train_tokenizer_phomt.py
  ```

  Output: `tokenizer_models/` with vocabulary and model files.

- **Download & preprocess dataset:**

  ```python
  from utils.data_processing import DataProcessor
  processor = DataProcessor(config)
  processor.download_and_prepare_phomt()
  ```

  Output: `data/processed/` with train/validation/test splits in `.en`/`.vi` files.

- **Tokenize and cache datasets:**
  This is done automatically when running `train.py` for the first time.

### 2. Train Model

```bash
python trainer/train.py
```

Output: Model checkpoints in `checkpoints/best_model_vi2en/`.

### 3. Inference (Translation)

- **Batch translate test set:**

  ```bash
  python translate_test_file.py
  ```

  Output: `data/processed/test_predict.vi` and `ep13_test_predict.jsonl`.

- **(Optional) Fast batch inference:**
  ```bash
  python translate_from_tokenized.py --input data/processed/test_tokenized.pkl --output data/processed/test_predict.vi
  ```

### 4. Evaluation

```bash
python evaluate_jsonl.py
```

Output: BLEU scores and evaluation summary in `data/processed/ep13_evaluation_results.json`.

---

## ğŸ”§ Advanced Usage

### Custom Configuration

```python
from models_best import TransformerConfig

config = TransformerConfig(
    d_model=512,
    n_encoder_layers=6,
    n_decoder_layers=6,
    n_heads=8,
    d_ff=2048,
    dropout=0.1,
    max_len=512,
    learning_rate=1e-4,
    warmup_steps=8000,
    label_smoothing=0.1
)
```

## ğŸ¯ Results

| Model | BLEU (enâ†’vi) |
| ----- | ------------ |
| Base  | ~38          |

### Special Tokens

- PAD: 0
- UNK: 1
- SOS: 2 (Start of Sequence)
- EOS: 3 (End of Sequence)

## ğŸ“š References

- **Attention Is All You Need** - Vaswani et al. (2017)
- **Pre-LN Transformer** - Xiong et al. (2020)
- **Label Smoothing** - Szegedy et al. (2016)
- **SentencePiece** - Kudo & Richardson (2018)

## ğŸ¤ Contributing

Feel free to:

- Report bugs
- Suggest features
- Submit pull requests
- Improve documentation

## ğŸ“„ License

MIT License - See LICENSE file for details

---

**Happy Translating! ğŸŒ**

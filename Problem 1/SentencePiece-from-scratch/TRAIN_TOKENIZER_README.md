# Training Tokenizer on PhoMT Dataset

HÆ°á»›ng dáº«n train vÃ  sá»­ dá»¥ng SentencePiece tokenizer tá»« scratch trÃªn dataset PhoMT (Vietnamese-English).

## ğŸ“‹ Tá»•ng Quan

Repository nÃ y implement tokenizer SentencePiece hoÃ n toÃ n tá»« Ä‘áº§u vá»›i **2-stage approach**:

1. **Stage 1 - Byte Pair Encoding (BPE)**: Táº¡o vocabulary ban Ä‘áº§u báº±ng cÃ¡ch gá»™p cÃ¡c cáº·p kÃ½ tá»±/token xuáº¥t hiá»‡n nhiá»u nháº¥t
2. **Stage 2 - EM Algorithm vá»›i Unigram Probabilities**: Nháº­n tokens tá»« BPE, sau Ä‘Ã³:
   - TÃ­nh unigram log-probabilities cho má»—i token
   - DÃ¹ng EM algorithm Ä‘á»ƒ tÃ¬m tokenization tá»‘i Æ°u
   - Pruning Ä‘á»ƒ giáº£m vocabulary vá» kÃ­ch thÆ°á»›c mong muá»‘n
   - Há»— trá»£ n-best sampling (stochastic tokenization)

**LÆ°u Ã½**: ÄÃ¢y KHÃ”NG pháº£i lÃ  Unigram Language Model thuáº§n tÃºy nhÆ° Google SentencePiece. Thay vÃ o Ä‘Ã³, nÃ³ káº¿t há»£p BPE Ä‘á»ƒ táº¡o vocabulary vá»›i EM Ä‘á»ƒ tá»‘i Æ°u tokenization.

## ğŸš€ CÃ i Äáº·t

### YÃªu cáº§u

```bash
pip install datasets tqdm scipy numpy
```

### Cáº¥u trÃºc files

```
SentencePiece-from-scratch/
â”œâ”€â”€ byte_pair_encoder.py          # BPE implementation
â”œâ”€â”€ sentence_piece.py              # Unigram LM vá»›i EM algorithm
â”œâ”€â”€ train_tokenizer_phomt.py      # Script train tokenizer trÃªn PhoMT
â”œâ”€â”€ test_tokenizer.py              # Script test tokenizer
â””â”€â”€ TRAIN_TOKENIZER_README.md     # File nÃ y
```

## ğŸ“š CÃ¡ch Sá»­ Dá»¥ng

### 1. Train Tokenizer trÃªn PhoMT

```bash
python train_tokenizer_phomt.py
```

**Cáº¥u hÃ¬nh máº·c Ä‘á»‹nh:**

- Vocabulary size: 32,000 tokens
- BPE merges: 10,000 operations
- Training samples: 500,000 pairs (cÃ³ thá»ƒ Ä‘á»•i thÃ nh `None` Ä‘á»ƒ dÃ¹ng toÃ n bá»™ ~2.9M)
- Output directory: `./tokenizer_models/`

**CÃ¡c bÆ°á»›c thá»±c hiá»‡n:**

1. **Download dataset** tá»« HuggingFace (`ura-hcmut/PhoMT`)
2. **Normalize vÃ  filter** dá»¯ liá»‡u
3. **Train BPE** - Gá»™p cÃ¡c cáº·p kÃ½ tá»±/token xuáº¥t hiá»‡n nhiá»u nháº¥t
4. **Train Unigram LM** - Sá»­ dá»¥ng EM algorithm Ä‘á»ƒ tá»‘i Æ°u vocabulary
5. **Pruning** - Giáº£m vocabulary vá» kÃ­ch thÆ°á»›c mong muá»‘n
6. **Save models** - LÆ°u tokenizer vÃ  vocabulary

**Output files:**

```
tokenizer_models/
â”œâ”€â”€ bpe_encoder.pkl              # BPE encoder model
â”œâ”€â”€ sentencepiece_trainer.pkl    # SentencePiece trainer model
â”œâ”€â”€ vocabulary.txt               # Vocabulary list (human-readable)
â””â”€â”€ metadata.txt                 # Training metadata
```

### 2. Test Tokenizer

```bash
python test_tokenizer.py
```

Script nÃ y sáº½:

- Load tokenizer Ä‘Ã£ train
- Test trÃªn cÃ¢u tiáº¿ng Anh
- Test trÃªn cÃ¢u tiáº¿ng Viá»‡t
- Test trÃªn cÃ¢u mixed language
- Test edge cases
- So sÃ¡nh n-best tokenizations
- PhÃ¢n tÃ­ch vocabulary statistics
- Cháº¿ Ä‘á»™ interactive Ä‘á»ƒ test cÃ¢u tÃ¹y Ã½

### 3. Sá»­ dá»¥ng trong Code

```python
import pickle
from sentence_piece import SentencePieceTrainer

# Load tokenizer
with open('tokenizer_models/sentencepiece_trainer.pkl', 'rb') as f:
    tokenizer = pickle.load(f)

# Tokenize text
text = "Hello world! Xin chÃ o tháº¿ giá»›i!"
tokens = tokenizer.tokenize(text, nbest_size=1)
print(tokens)
# Output: ['Hello', '_world', '!', '_Xin', '_chÃ o', '_tháº¿', '_giá»›i', '!']
```

## ğŸ”§ TÃ¹y Chá»‰nh Cáº¥u HÃ¬nh

Chá»‰nh sá»­a trong `train_tokenizer_phomt.py`:

```python
# Configuration
VOCAB_SIZE = 32000        # KÃ­ch thÆ°á»›c vocabulary mong muá»‘n
NUM_BPE_MERGES = 10000    # Sá»‘ lÆ°á»£ng BPE merge operations
MAX_SAMPLES = 500000      # Sá»‘ lÆ°á»£ng samples (None = toÃ n bá»™)
OUTPUT_DIR = './tokenizer_models'

trainer = PhomtTokenizerTrainer(
    vocab_size=VOCAB_SIZE,
    num_bpe_merges=NUM_BPE_MERGES,
    max_samples=MAX_SAMPLES,
    output_dir=OUTPUT_DIR
)
```

### Gá»£i Ã½ cáº¥u hÃ¬nh:

| Use Case     | Vocab Size | BPE Merges | Samples     |
| ------------ | ---------- | ---------- | ----------- |
| Quick test   | 8,000      | 2,000      | 50,000      |
| Small model  | 16,000     | 5,000      | 200,000     |
| **Default**  | **32,000** | **10,000** | **500,000** |
| Large model  | 50,000     | 20,000     | 1,000,000   |
| Full dataset | 64,000     | 30,000     | None (all)  |

## ğŸ“Š Thuáº­t ToÃ¡n Chi Tiáº¿t

### Stage 1: Byte Pair Encoding (BPE)

```
1. Initialize: Má»—i kÃ½ tá»± lÃ  má»™t token
2. Count: Äáº¿m táº§n suáº¥t táº¥t cáº£ cÃ¡c bigrams
3. Merge: Gá»™p bigram xuáº¥t hiá»‡n nhiá»u nháº¥t
4. Repeat: Láº·p láº¡i N láº§n (num_merges)
```

**VÃ­ dá»¥:**

```
Input:  "hello world"
Step 0: h e l l o _ w o r l d _
Step 1: h e ll o _ w o r l d _      (merge l+l)
Step 2: h e ll o _ w o r ll d _     (merge l+l again)
Step 3: he ll o _ w o r ll d _      (merge h+e)
...
```

### Stage 2: EM Algorithm vá»›i Unigram Probabilities

**Quan trá»ng**: ÄÃ¢y khÃ´ng pháº£i lÃ  Unigram LM thuáº§n tÃºy. Tokens Ä‘Ã£ Ä‘Æ°á»£c táº¡o sáºµn tá»« BPE, vÃ  EM chá»‰ dÃ¹ng Ä‘á»ƒ optimize probabilities vÃ  tÃ¬m tokenization tá»‘t nháº¥t.

```
E-step (Expectation):
  - Tokenize text vá»›i probabilities hiá»‡n táº¡i
  - Äáº¿m táº§n suáº¥t tokens trong tokenization
  - Update probabilities trong Trie

M-step (Maximization):
  - TÃ¬m tokenization tá»‘i Æ°u báº±ng Viterbi (Dynamic Programming)
  - Maximize log-likelihood cá»§a tokenization

Pruning:
  - XÃ³a 20% tokens Ã­t xuáº¥t hiá»‡n nháº¥t (trá»« base characters)
  - Láº·p láº¡i cho Ä‘áº¿n khi Ä‘áº¡t vocab_size
```

**Forward Step (Viterbi):**

```python
# d[i] = max log-prob cá»§a tokenization cho text[:i]
# p[i] = Ä‘á»™ dÃ i token cuá»‘i cÃ¹ng trong cÃ¡ch tokenize tá»‘t nháº¥t

for i in range(1, N+1):
    for j in range(max(i-maxlen, 0), i):
        token = text[j:i]
        if d[j] + prob(token) > d[i]:
            d[i] = d[j] + prob(token)
            p[i] = len(token)
```

## ğŸ¯ VÃ­ Dá»¥ Output

### Tokenization Examples

```
Input:  "Machine learning is amazing."
Tokens: ['â–Machine', 'â–learning', 'â–is', 'â–amazing', '.']

Input:  "Xin chÃ o tháº¿ giá»›i!"
Tokens: ['â–Xin', 'â–chÃ o', 'â–tháº¿', 'â–giá»›i', '!']

Input:  "I study at Äáº¡i há»c BÃ¡ch Khoa."
Tokens: ['â–I', 'â–study', 'â–at', 'â–Äáº¡i', 'â–há»c', 'â–BÃ¡ch', 'â–Khoa', '.']
```

(LÆ°u Ã½: `â–` lÃ  kÃ½ hiá»‡u thay tháº¿ cho `_` Ä‘á»ƒ dá»… nhÃ¬n, Ä‘áº¡i diá»‡n cho dáº¥u cÃ¡ch)

### Top Frequent Tokens

```
1. 'â–' â†’ 2,456,789 occurrences
2. 'e' â†’ 1,234,567 occurrences
3. 't' â†’ 987,654 occurrences
4. 'â–the' â†’ 456,789 occurrences
5. 'ing' â†’ 345,678 occurrences
...
```

## ğŸ” So SÃ¡nh vá»›i SentencePiece ChÃ­nh Thá»©c

| Feature                   | Implementation nÃ y   | Google SentencePiece     |
| ------------------------- | -------------------- | ------------------------ |
| **Vocabulary Generation** | BPE only             | BPE hoáº·c Unigram LM      |
| **Tokenization**          | EM vá»›i unigram probs | BPE hoáº·c Unigram LM      |
| **Approach**              | 2-stage (BPE â†’ EM)   | Single algorithm         |
| EM Algorithm              | âœ… CÃ³ (Ä‘á»ƒ optimize)  | âœ… CÃ³ (náº¿u dÃ¹ng Unigram) |
| Pruning                   | âœ… CÃ³                | âœ… CÃ³                    |
| N-best sampling           | âœ… CÃ³                | âœ… CÃ³                    |
| Subword regularization    | âš ï¸ Basic             | âœ… Advanced              |
| Character coverage        | âŒ KhÃ´ng             | âœ… CÃ³                    |
| Performance               | ğŸŒ Cháº­m (Python)     | âš¡ Nhanh (C++)           |

**Äiá»ƒm khÃ¡c biá»‡t chÃ­nh**:

- Google SentencePiece cho phÃ©p chá»n **hoáº·c** BPE **hoáº·c** Unigram LM
- Implementation nÃ y **káº¿t há»£p cáº£ hai**: BPE táº¡o tokens, EM optimize tokenization

## ğŸ“ˆ Performance Notes

**Training time** (trÃªn CPU, 500k samples):

- BPE: ~5-10 phÃºt
- Unigram EM: ~10-20 phÃºt
- Total: ~15-30 phÃºt

**Memory usage:**

- ~2-4 GB RAM cho 500k samples
- ~8-16 GB RAM cho full dataset (~2.9M)

## ğŸ› Troubleshooting

### Lá»—i: Out of Memory

```python
# Giáº£m sá»‘ lÆ°á»£ng samples
MAX_SAMPLES = 100000  # Thay vÃ¬ 500000
```

### Lá»—i: Tokenization fails

```python
# Kiá»ƒm tra xem token cÃ³ trong vocabulary khÃ´ng
# CÃ³ thá»ƒ cáº§n tÄƒng num_bpe_merges hoáº·c giáº£m vocab_size
```

### Lá»—i: Dataset download slow

```bash
# Set HuggingFace cache directory
export HF_HOME=/path/to/cache
```

## ğŸ“š TÃ i Liá»‡u Tham Kháº£o

1. **SentencePiece**: [GitHub Repository](https://github.com/google/sentencepiece)
2. **BPE Paper**: Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2016)
3. **Unigram LM**: Subword Regularization (Kudo, 2018)
4. **PhoMT Dataset**: [HuggingFace](https://huggingface.co/datasets/ura-hcmut/PhoMT)

## ğŸ“ Educational Purpose

Code nÃ y Ä‘Æ°á»£c viáº¿t vá»›i má»¥c Ä‘Ã­ch **há»c táº­p vÃ  hiá»ƒu sÃ¢u** vá» cÃ¡ch tokenizer hoáº¡t Ä‘á»™ng:

- âœ… Dá»… Ä‘á»c vÃ  cÃ³ comment chi tiáº¿t
- âœ… Implement tá»« Ä‘áº§u, khÃ´ng dÃ¹ng thÆ° viá»‡n cÃ³ sáºµn
- âœ… CÃ³ visualization vÃ  logging rÃµ rÃ ng
- âŒ KhÃ´ng tá»‘i Æ°u cho production (dÃ¹ng Google SentencePiece thay tháº¿)

## ğŸ“ License

MIT License - Free to use for educational purposes

---

**Happy Tokenizing! ğŸš€**

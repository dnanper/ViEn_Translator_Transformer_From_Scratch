# ğŸ› Fix CUDA Index Out of Bounds Error

## âŒ Lá»—i gáº·p pháº£i:

```
CUDA error: device-side assert triggered
vectorized_gather_kernel: index out of bounds
```

## ğŸ” NguyÃªn nhÃ¢n:

Token IDs trong data **>= vocab_size** â†’ Embedding layer khÃ´ng thá»ƒ lookup

VÃ­ dá»¥:

- Vocab size = 32,001 (model biáº¿t)
- Token ID = 32,001 hoáº·c lá»›n hÆ¡n (trong data)
- â†’ Index out of bounds!

---

## âœ… CÃ¡ch Fix TrÃªn Colab

### BÆ°á»›c 1: Cháº¡y script debug

```python
# Cell 1: Debug token IDs
!cd /content/ViEn_Translator_Transformer_From_Scratch && python debug_tokens.py
```

Script nÃ y sáº½:

- Scan toÃ n bá»™ data
- TÃ¬m max token ID
- So sÃ¡nh vá»›i vocab_size
- BÃ¡o cÃ¡o náº¿u cÃ³ váº¥n Ä‘á»

### BÆ°á»›c 2: Cháº¡y quick fix

```python
# Cell 2: Apply quick fix
!cd /content/ViEn_Translator_Transformer_From_Scratch && python colab_quick_fix.py
```

Script nÃ y sáº½ cho báº¡n biáº¿t:

- Vocab size Ä‘Ãºng lÃ  bao nhiÃªu
- Cáº§n sá»­a gÃ¬ trong train.py

### BÆ°á»›c 3: Patch code trá»±c tiáº¿p

```python
# Cell 3: Patch train.py
import re

file_path = '/content/ViEn_Translator_Transformer_From_Scratch/trainer/train.py'

with open(file_path, 'r') as f:
    content = f.read()

# Find actual vocab size from debug output
# Let's say it's 32001 (adjust based on colab_quick_fix.py output)
ACTUAL_VOCAB_SIZE = 32001  # â† THAY Äá»”I Náº¾U Cáº¦N

# Option A: Fix by clamping token IDs (safe but slower)
old_line = "            # Validate token IDs (é˜²æ­¢ index out of bounds)"
new_code = f"""            # Validate token IDs (é˜²æ­¢ index out of bounds)
            vocab_size = {ACTUAL_VOCAB_SIZE}
            src = torch.clamp(src, 0, vocab_size - 1)
            tgt = torch.clamp(tgt, 0, vocab_size - 1)"""

if old_line not in content:
    # Add the validation code
    search = "            src = batch['src'].to(self.config.device)"
    replace = f\"\"\"            src = batch['src'].to(self.config.device)
            tgt_original = batch['tgt'].to(self.config.device)

            # Validate token IDs
            vocab_size = {ACTUAL_VOCAB_SIZE}
            src = torch.clamp(src, 0, vocab_size - 1)
            tgt = torch.clamp(tgt_original, 0, vocab_size - 1)\"\"\"

    content = content.replace(search, replace)
    content = content.replace(
        "tgt = batch['tgt'].to(self.config.device)",
        "# tgt moved above with validation"
    )

with open(file_path, 'w') as f:
    f.write(content)

print("âœ… Patched train.py with token ID validation")
```

---

## ğŸ”§ Giáº£i PhÃ¡p Chi Tiáº¿t

### Giáº£i phÃ¡p 1: Clamp token IDs (Nhanh)

ThÃªm vÃ o `train_epoch()` trong `trainer/train.py`:

```python
# After loading batch to device
src = batch['src'].to(self.config.device)
tgt = batch['tgt'].to(self.config.device)

# Clamp to valid range
vocab_size = self.model.src_vocab_size
src = torch.clamp(src, 0, vocab_size - 1)
tgt = torch.clamp(tgt, 0, vocab_size - 1)
```

**Æ¯u Ä‘iá»ƒm:** Fix ngay láº­p tá»©c  
**NhÆ°á»£c Ä‘iá»ƒm:** KhÃ´ng giáº£i quyáº¿t nguyÃªn nhÃ¢n gá»‘c

### Giáº£i phÃ¡p 2: Fix vocab_size (ÄÃºng Ä‘áº¯n)

1. **Kiá»ƒm tra vocab tháº­t sá»±:**

```python
# Count lines in vocabulary.txt
!wc -l /content/ViEn_Translator_Transformer_From_Scratch/SentencePiece-from-scratch/tokenizer_models/vocabulary.txt
```

2. **Update metadata.txt:**

```python
# Update vocab_size in metadata
metadata_file = '/content/ViEn_Translator_Transformer_From_Scratch/SentencePiece-from-scratch/tokenizer_models/metadata.txt'

with open(metadata_file, 'r') as f:
    lines = f.readlines()

new_lines = []
for line in lines:
    if line.startswith('vocab_size'):
        new_lines.append(f'vocab_size = 32001\n')  # Or actual vocab size
    else:
        new_lines.append(line)

with open(metadata_file, 'w') as f:
    f.writelines(new_lines)

print("âœ… Updated metadata.txt")
```

3. **Reload tokenizer:**

```python
# Restart Python kernel and re-run training
# Or manually reload processor
```

### Giáº£i phÃ¡p 3: Re-tokenize data (Triá»‡t Ä‘á»ƒ)

Náº¿u data bá»‹ corrupt:

```python
from utils.data_processing import DataProcessor
from config import Config
import os

processor = DataProcessor(Config)
tokenizer_dir = '/content/ViEn_Translator_Transformer_From_Scratch/SentencePiece-from-scratch/tokenizer_models'
processor.load_tokenizer(tokenizer_dir)

# Delete old cached files
import os
data_dir = '/content/ViEn_Translator_Transformer_From_Scratch/data/processed'
for f in ['train_tokenized.pkl', 'validation_tokenized.pkl', 'test_tokenized.pkl']:
    path = os.path.join(data_dir, f)
    if os.path.exists(path):
        os.remove(path)
        print(f"Deleted {f}")

# Re-tokenize
datasets = processor.prepare_datasets()
print("âœ… Re-tokenized all data")
```

---

## ğŸ“ Temporary Fix (Äá»ƒ train ngay)

Copy Ä‘oáº¡n code nÃ y vÃ o cell trÆ°á»›c khi train:

```python
# EMERGENCY FIX - Run this before training
import torch

# Monkey patch Embedding to clamp indices
original_embedding_forward = torch.nn.Embedding.forward

def safe_embedding_forward(self, input):
    # Clamp input to valid range
    input = torch.clamp(input, 0, self.num_embeddings - 1)
    return original_embedding_forward(self, input)

torch.nn.Embedding.forward = safe_embedding_forward
print("âœ… Patched Embedding layer to auto-clamp indices")

# Now run training
!python trainer/train.py
```

âš ï¸ **Warning:** ÄÃ¢y chá»‰ lÃ  quick fix! NÃªn tÃ¬m vÃ  fix nguyÃªn nhÃ¢n gá»‘c.

---

## ğŸ¯ Checklist

- [ ] Cháº¡y `debug_tokens.py` Ä‘á»ƒ tÃ¬m max token ID
- [ ] Cháº¡y `colab_quick_fix.py` Ä‘á»ƒ xÃ¡c Ä‘á»‹nh vocab_size Ä‘Ãºng
- [ ] So sÃ¡nh:
  - Vocab size in metadata: `___`
  - Actual vocab lines: `___`
  - Max token ID in data: `___`
- [ ] Apply fix (chá»n 1):
  - [ ] Clamp token IDs (temporary)
  - [ ] Update metadata.txt (proper)
  - [ ] Re-tokenize data (thorough)
- [ ] Re-run training

---

## ğŸ’¡ Prevent Future Issues

Trong `data_processing.py`, thÃªm validation:

```python
def encode_sentence(self, text, add_sos=False, add_eos=False):
    # ... existing code ...

    # Validate token IDs
    max_id = max(token_ids) if token_ids else 0
    if max_id >= self.vocab_size:
        print(f"âš ï¸  Warning: Token ID {max_id} >= vocab_size {self.vocab_size}")
        # Clip to valid range
        token_ids = [min(tid, self.vocab_size - 1) for tid in token_ids]

    return token_ids
```

---

ChÃºc báº¡n fix bug thÃ nh cÃ´ng! ğŸš€
